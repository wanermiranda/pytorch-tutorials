{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56739ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import sys \n",
    "sys.path.append('../')\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "275b8e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wompth.models.dqn import Transition, ReplayMemory, DQN, ScreenDims, DQNConf, fit_networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a5ec6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gorigan/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAACICAYAAAD+r7D/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAASbUlEQVR4nO3de7RU5XnH8e+P4XDzBggSwkVMxBhiDVgacSVtbMSIaQzpqk20qZdoYrqaVO0yFy9dibYxjSsmxqzcZJWIVeslXgk1UYKQNEmr4gUvIIpGAwTkIkcwGDgHnv6x34ObYYYzcM6Zmc35fdaadWa/7569n/2emWfeefdNEYGZmRVPn0YHYGZme8cJ3MysoJzAzcwKygnczKygnMDNzArKCdzMrKCcwK3uJJ0t6VeNjqOZuE1sbziB72MkvSTpDUmv5x7fbXRcjSbpckk39eDyF0j6VE8t36ySvo0OwHrEKRHx80YHUSSSBCgitjc6lp4gqW9EtDc6Dute7oH3IpJ+IOnO3PRVkuYpM0TSHElrJW1Iz0fn5l0g6auSfpN69T+RdLCkmyVtlPSIpHG5+UPS+ZJelLRO0jckVXy/STpS0lxJr0paKulju9mGgyTNlLRK0soUU0lSP0lPSPqnNF9J0q8lfVnSNOBS4OMp9kW5bbpS0q+BzcDbJH1S0hJJm1Lsnylb//S0no2SXpA0TdKVwJ8D383/4tnddqW2m52W8zDw9t1s8wBJN0laL6k1tfWIVDdU0vWSfp/+b/ek8uMlrZD0JUmrgesl9ZF0cYp7vaTbJQ3NrWdK+v+2Slok6fiy//+/pTbdJOkBScOqxWx1EhF+7EMP4CVgapW6QcBzwNlkCWcdMDrVHQz8TZrnAODHwD251y4AlpElmoOAxWlZU8l+yf0ncH1u/gDmA0OBsWneT6W6s4Ffpef7AcuBT6blTEpxTaiyDXcD16XXHQI8DHwm1R0FbADeCVwG/B9QSnWXAzeVLWsB8DvgXWndLcBfpW0U8H6yxH5Mmv89wGvAiWSdn1HAkbllfSq37N1uF3ArcHua7yhgZUebVNjmzwA/Sf+bEvCnwIGp7r+B24AhKf73p/LjgXbgKqA/MBC4ILXJ6FR2HXBLmn8UsB74UNq2E9P08Nz2vQAckZa1APh6o9/vvf3R8AD86OZ/aJbAXwdac49P5+qPBV4FXgZO381yJgIbctMLgMty098EfpqbPgV4IjcdwLTc9D8C89Lzs3kzgX8c+J+ydV8HfKVCTCOALcDAXNnpwPzc9EXAUrJEPj5XfjmVE/i/dtKe9wAX5OK6psp8C9g5gVfdrpSE20jJP9V9jeoJ/BzgN8DRZeUjge3AkAqvOR7YCgzIlS0BTih7fRvZF8yXgBvLlnE/cFZu+/6l7P/5s0a/33v7w2Pg+6aPRpUx8Ih4SNKLZL3X2zvKJQ0CrgGmkfXmAA6QVIqIbWn6ldyi3qgwvX/Z6pbnnr8MvLVCSIcCx0pqzZX1BW6sMm8LsCobsgay3mJ+PTcAVwJ3RsTzFZZRLv9aJJ1MlmSPSMseBDyVqscA99WwzI5Yq23X8PS8vH2quTGt+1ZJg4GbyH5hjAFejYgNVV63NiL+WBbT3ZLy4/zbyL4YDwX+VtIpuboWsl9RHVbnnm9m1/+31ZkTeC8j6bNkP59/D3wR+PdUdRHwDuDYiFgtaSLwONlQwt4aAzyTno9N6yy3HPhFRJxYw/KWk/XAh0X1HXLfB+YAJ0l6X0R0HJpX7bKbO8ol9QfuBM4E7o2ItjSm3NEGy6k+Vl2+/KrbJalENrwxBng2FY+tslwiog24Argi7We4j+xXxn3AUEmDI6K1xpjOiYhfV4hpOVkP/NPV4rDm452YvYikI4CvAn8PnAF8MSVqyMa93wBa046tr3TDKr+Qdo6OIRt/va3CPHOAIySdIaklPf5M0jvLZ4yIVcADwDclHZh2yr1d0vvT9p1BNj58NnA+cIOkjl7iK8C4ajtSk35kX25rgfbUG/9grn4m8ElJJ6R1j5J0ZG75b6tlu9IvmruAyyUNkjQBOKtaUJL+UtKfpMS/kWzYY3tqj58C30/t3CLpL3azfT8ErpR0aFrucEnTU91NwCmSTlK2A3hA2hE6uurSrOGcwPdNP9HOx4HfLakv2Yf0qohYlIYXLgVuTD3Pb5PtnFpHtqPrZ90Qx73Ao8ATZDvbZpbPEBGbyJLkaWQ99NW8ueOtkjPJEu1isnHuO4CRksambTgzIl6PiP8CFpINC0G2UxZgvaTHKi04xXI+2dDSBuDvgNm5+ofJdkpeQ7Yz8xdkQw8A1wKnpiNBvlPDdn2ObAhiNTALuL7K9gK8JW3nRrJx7F/w5hDTGWQJ/VlgDXDhbpZzbdqeByRtIvs/H5u2bTkwnew9sZast/4FnCOamtIOCbNuJSnIdiIua3QsZvsqf7uamRWUE7iZWUF1KYGns9CWSlom6eLuCsqKLyLk4ROznrXXY+Bpj/hzZGdsrQAeITsxZHH3hWdmZtV0pQf+HmBZRLwYEVvJTg2e3slrzMysm3TlRJ5R7Hwm2QrSIUnVDBs2LMaNG9eFVZqZ9T6PPvrouogYXl7e42diSjoPOA9g7NixLFy4sKdXaWa2T5FU8VILXRlCWUl2KnCH0alsJxExIyImR8Tk4cN3+QIxM7O91JUE/ggwXtJhkvqRnXE2u5PXmDWl7dvadjzMimKvh1Aiol3S58guOVkCfhQRz3TyMjMz6yZdGgOPiPuo/fKaZmbWjXw5WTPg5V++eb/jLa9llznf/y2HAzB6yqkNicmsMz6V3sysoNwDNwO2vv7qjuetLy8CQH1KjQrHrCbugZuZFZR74Na7VbgWUJ9SCwDZ5X7Mmpd74GZmBeUeuPVq7Vs3A7CldfUudYOG+XaQ1tzcAzczKygncDOzgvIQivVuaSdmxPZdqkr996t3NGZ7xD1wM7OCcg/crJq9vN2gWb24B25mVlBO4GZmBdVpApf0I0lrJD2dKxsqaa6k59PfIT0bppmZlaulBz4LmFZWdjEwLyLGA/PStFkBKT0qiPA4uDW1ThN4RPwSeLWseDpwQ3p+A/DR7g3LzMw6s7dj4CMiYlV6vhoYUW1GSedJWihp4dq1a/dydWZmVq7LhxFGREiq+jszImYAMwAmT57s36PWVLZsWgNA+5Y/7ChTKftYDPS1UKzJ7W0P/BVJIwHS3zXdF5KZmdVibxP4bOCs9Pws4N7uCcesvrZv/SPbt/6R2Na+4yH1QepDqf8gSv0HNTpEs6pqOYzwFuB/gXdIWiHpXODrwImSngempmkzM6ujTsfAI+L0KlUndHMsZvWnKocQgg8htKbnMzHNzArKCdzMrKCcwM3MCsoJ3MysoHw9cOvVpEp9mLTz0jsxrcm5B25mVlDugVuvtmXTOgBi+7YdZX3TvTBbBh3UkJjMauUeuJlZQbkHbr3alo279sD7tAwAoK974Nbk3AM3Myso98CtV/NRKFZk7oGbmRWUE7iZWUHVcjnZMZLmS1os6RlJF6Ry35nezKyBaumBtwMXRcQEYArwWUkT8J3pzcwaqpa70q+KiMfS803AEmAUvjO97QukXa8JHuEdmFYIezQGLmkcMAl4iBrvTO+70puZ9YyaE7ik/YE7gQsjYmO+LiKCHcde7SwiZkTE5IiYPHz48C4Fa9bdtm35A9tyd6QH6NNvAH36DUB9SqhPqUGRmXWupgQuqYUsed8cEXelYt+Z3sysgWo5CkXATGBJRHwrV+U701vhbV63gs3rVuxU1v+AQ+h/wCGU+g2k1G9ggyIz61wtZ2K+FzgDeErSE6nsUrI70d+e7lL/MvCxHonQzMwqquWu9L8Cqt2623emNzNrEF8LxXq38kMIgSr7482ajk+lNzMrKCdwM7OCcgI3MysoJ3Azs4JyAjczKygfhWK9U7pYVcT2Xap8+rwVhXvgZmYF5QRuZlZQHkKxXql962YAtrSu3qVuv+GH1jscs73iHriZWUG5B269k3di2j7APXAzs4Kq5XrgAyQ9LGlRuiv9Fan8MEkPSVom6TZJ/Xo+XDMz61BLD3wL8IGIeDcwEZgmaQpwFXBNRBwObADO7bEozcxsF7XclT4i4vU02ZIeAXwAuCOV+670Vih9W1ro29KCFEgdt3TNHqVSiVLJ4+DW/Gq9J2Yp3Y1nDTAXeAFojYj2NMsKYFSPRGhmZhXVlMAjYltETARGA+8Bjqx1BZLOk7RQ0sK1a9fuXZRmZraLPTqMMCJaJc0HjgMGS+qbeuGjgZVVXjMDmAEwefJk3+rEeszjjz8OwOc///lO592vf9Z3ufCkwwEYesCQHXWzZs0E4P6nru50OVdfnc0zadKkPQvWrBvUchTKcEmD0/OBwInAEmA+cGqazXelNzOrs1p64COBGySVyBL+7RExR9Ji4FZJXwUeB2b2YJxmnVq/fj0ADz74YKfzjh7xVgAWHncJAP3e6L+j7rYH/gGAx5Y8XfM6zRqhlrvSPwns8vswIl4kGw83M7MG8Kn0ts/o27f2t/OBBw5NLzoYgDa17KjbtLWl0ku6vE6z7uZT6c3MCqqu3Ye2tjZWrVpVz1VaL7Ju3bqa512x8jkAbph1DgATxh2yo27za8v2eJ1+X1sjuAduZlZQTuBmZgVV1yGU9vZ2fDam9ZTW1taa5924eSsAi597LP3t2jr9vrZGcA/czKyg6toDHzhwIEcffXQ9V2m9yIYNG+q+zvHjxwP4fW0N4R64mVlB+SwE22e0tbX1inWadXAP3MysoNwDt33GsGHDAJg6dWrd12nWCO6Bm5kVlBO4mVlBeQjF9hkTJ04EYO7cuY0NxKxO3AM3MysoRdTvNpWS1gJ/AGq/bFxzGEaxYi5avOCY66Fo8ULxYu6peA+NiOHlhXVN4ACSFkbE5LqutIuKFnPR4gXHXA9FixeKF3O94/UQiplZQTmBm5kVVCMS+IwGrLOrihZz0eIFx1wPRYsXihdzXeOt+xi4mZl1Dw+hmJkVVN0SuKRpkpZKWibp4nqtd09IGiNpvqTFkp6RdEEqHypprqTn098hjY41T1JJ0uOS5qTpwyQ9lNr6Nkn9Gh1jnqTBku6Q9KykJZKOK0Ab/3N6Tzwt6RZJA5qtnSX9SNIaSU/nyiq2qzLfSbE/KemYJon3G+l98aSkuyUNztVdkuJdKumkesdbLeZc3UWSQtKwNN3jbVyXBC6pBHwPOBmYAJwuaUI91r2H2oGLImICMAX4bIrzYmBeRIwH5qXpZnIBsCQ3fRVwTUQcDmwAzm1IVNVdC/wsIo4E3k0We9O2saRRwPnA5Ig4CigBp9F87TwLmFZWVq1dTwbGp8d5wA/qFGPeLHaNdy5wVEQcDTwHXAKQPoenAe9Kr/l+yiv1NotdY0bSGOCDwO9yxT3fxhHR4w/gOOD+3PQlwCX1WHcX474XOBFYCoxMZSOBpY2OLRfjaLIP5geAOYDITiToW6ntG/0ADgJ+S9r/kitv5jYeBSwHhpJdfmIOcFIztjMwDni6s3YFrgNOrzRfI+Mtq/tr4Ob0fKecAdwPHNcMbZzK7iDrjLwEDKtXG9drCKXjA9BhRSprWpLGAZOAh4AREbEqVa0GRjQqrgq+DXwR2J6mDwZaI6I9TTdbWx8GrAWuT8M+/yFpP5q4jSNiJXA1We9qFfAa8CjN3c4dqrVrET6T5wA/Tc+bNl5J04GVEbGorKrHY/ZOzAok7Q/cCVwYERvzdZF9lTbFoTuSPgysiYhHGx3LHugLHAP8ICImkV1aYafhkmZqY4A0bjyd7MvnrcB+VPgZ3eyarV13R9JlZEOaNzc6lt2RNAi4FPhyI9ZfrwS+EhiTmx6dypqOpBay5H1zRNyVil+RNDLVjwTWNCq+Mu8FPiLpJeBWsmGUa4HBkjquNNlsbb0CWBERD6XpO8gSerO2McBU4LcRsTYi2oC7yNq+mdu5Q7V2bdrPpKSzgQ8Dn0hfOtC88b6d7It9UfocjgYek/QW6hBzvRL4I8D4tNe+H9nOiNl1WnfNJAmYCSyJiG/lqmYDZ6XnZ5GNjTdcRFwSEaMjYhxZmz4YEZ8A5gOnptmaJl6AiFgNLJf0jlR0ArCYJm3j5HfAFEmD0nukI+ambeecau06GzgzHSkxBXgtN9TSMJKmkQ0JfiQiNueqZgOnSeov6TCyHYMPNyLGvIh4KiIOiYhx6XO4Ajgmvc97vo3rOPD/IbK9yi8AlzVi50MNMb6P7Cfmk8AT6fEhsnHlecDzwM+BoY2OtULsxwNz0vO3kb25lwE/Bvo3Or6yWCcCC1M73wMMafY2Bq4AngWeBm4E+jdbOwO3kI3Rt5ElknOrtSvZzu7vpc/jU2RH2DRDvMvIxo07Pn8/zM1/WYp3KXBys7RxWf1LvLkTs8fb2GdimpkVlHdimpkVlBO4mVlBOYGbmRWUE7iZWUE5gZuZFZQTuJlZQTmBm5kVlBO4mVlB/T+N16NBwaFDigAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width)\n",
    "    slice_range = slice(view_width)\n",
    "#     cart_location = get_cart_location(screen_width)\n",
    "#     if cart_location < view_width // 2:\n",
    "#         slice_range = slice(view_width)\n",
    "#     elif cart_location > (screen_width - view_width // 2):\n",
    "#         slice_range = slice(-view_width, None)\n",
    "#     else:\n",
    "#         slice_range = slice(cart_location - view_width // 2,\n",
    "#                             cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3577f3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 40, 150])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_screen().cpu().squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d1b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f36bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "screen_dims = ScreenDims(screen_height, screen_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8581cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential(\n",
    "#     nn.Conv2d(3,32,kernel_size=3,padding=1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Conv2d(32,64,kernel_size=3,stride=1,padding=1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(2,2),\n",
    "\n",
    "#     nn.Conv2d(64,128,kernel_size=3,stride=1,padding=1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Conv2d(128,128,kernel_size=3,stride=1,padding=1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(2,2),\n",
    "\n",
    "#     nn.Conv2d(128,256,kernel_size=3,stride=1,padding=1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Conv2d(256,256,kernel_size=3,stride=1,padding=1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(2,2),\n",
    "\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(256*4*4,1024),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(1024,512),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(512,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c538ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential = nn.Sequential(\n",
    "#       nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5),\n",
    "#       nn.ReLU(),\n",
    "#       nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#       nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n",
    "#       nn.ReLU(),\n",
    "#       nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#       nn.Flatten(start_dim=1)  ,\n",
    "#       nn.Linear(in_features=15232, out_features=120),\n",
    "#       nn.ReLU(),\n",
    "#       nn.Linear(in_features=120, out_features=60),\n",
    "#       nn.ReLU(),\n",
    "#       nn.Linear(in_features=60, out_features=2)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95342692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential(get_screen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0af031ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 40, 150])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_screen().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ec143e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "class DQN_BackBone(DQN): \n",
    "    def __init__(\n",
    "        self,\n",
    "        device=\"cuda\",\n",
    "        conf: DQNConf = DQNConf(),\n",
    "        optimizer_partial=partial(optim.RMSprop),\n",
    "        memory=ReplayMemory(10000),\n",
    "        outputs=2\n",
    "    ):\n",
    "        super().__init__(device, conf, optimizer_partial, memory, outputs)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)        \n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(150)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(40)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
    "        self.out = nn.Linear(in_features=128, out_features=self._outputs)\n",
    "        self._send_to_device(device=device)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(self._device)\n",
    "       \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e3d1489",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = DQNConf(\n",
    "    BATCH_SIZE = 128,\n",
    "    GAMMA = 0.999,\n",
    "    EPS_START = 0.5,\n",
    "    TARGET_UPDATE = 10, \n",
    "    MAX_EPISODES = 2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bacb63a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "target_net = DQN_BackBone(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0465538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0229,  0.2476]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_net(get_screen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71d88ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_net._linear_input_size, target_net._epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cf7bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# target_net.load_state_dict(policy_net.state_dict())\n",
    "# target_net.eval()\n",
    "\n",
    "\n",
    "def moving_average_pth(x, w=10):\n",
    "    kernel = [1/w] * w\n",
    "    ts_tensor = torch.Tensor(x).reshape(1, 1, -1)\n",
    "    kernel_tensor = torch.Tensor(kernel).reshape(1, 1, -1)\n",
    "    return F.conv1d(ts_tensor, kernel_tensor).reshape(-1)\n",
    "\n",
    "def plot_durations(i_episode, episode_durations):\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel(f'Episode {i_episode}')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    means = moving_average_pth(durations_t, conf.TARGET_UPDATE)\n",
    "    plt.plot(means.numpy())\n",
    "    display.display(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58720399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 -200\n",
      "10 30 -190\n",
      "20 50 -180\n",
      "30 70 -170\n",
      "40 90 -160\n",
      "50 110 -150\n",
      "60 130 -140\n",
      "70 150 -130\n",
      "80 170 -120\n",
      "90 190 -110\n",
      "100 210 -100\n",
      "110 230 -90\n",
      "120 250 -80\n",
      "130 270 -70\n",
      "140 290 -60\n",
      "150 310 -50\n",
      "160 330 -40\n",
      "170 350 -30\n",
      "180 370 -20\n",
      "190 390 -10\n",
      "200 410 200\n",
      "210 430 210\n"
     ]
    }
   ],
   "source": [
    "def reward_function_step(done, t, step_reward=10, max_reward=200):\n",
    "    reward = 0\n",
    "\n",
    "    if done:\n",
    "        if t >= max_reward:\n",
    "            reward = t  # discounted steps\n",
    "        else: \n",
    "            reward = -(max_reward - t)\n",
    "    elif t >= step_reward: \n",
    "        step_bonus = ((t // step_reward)+1)*10\n",
    "        reward = step_bonus + t # promote the reward in steps \n",
    "    else: \n",
    "        reward = t\n",
    "\n",
    "    return reward \n",
    "for t in range(0, 220, 10):\n",
    "    print (t, reward_function_step(False, t), reward_function_step(True, t),) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92d19449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 -200\n",
      "10 10 -190\n",
      "20 20 -180\n",
      "30 30 -170\n",
      "40 40 -160\n",
      "50 50 -150\n",
      "60 60 -140\n",
      "70 70 -130\n",
      "80 80 -120\n",
      "90 90 -110\n",
      "100 100 -100\n",
      "110 110 -90\n",
      "120 120 -80\n",
      "130 130 -70\n",
      "140 140 -60\n",
      "150 150 -50\n",
      "160 160 -40\n",
      "170 170 -30\n",
      "180 180 -20\n",
      "190 190 -10\n",
      "200 200 200\n",
      "210 210 210\n"
     ]
    }
   ],
   "source": [
    "def reward_function_linear(done, t, max_reward=200):\n",
    "    reward = 0\n",
    "\n",
    "    if done:\n",
    "        if t >= max_reward:\n",
    "            reward = t  # discounted steps\n",
    "        else: \n",
    "            reward = -(max_reward - t)\n",
    "    else:\n",
    "        reward = t\n",
    "\n",
    "    return reward \n",
    "for t in range(0, 220, 10):\n",
    "    print (t, reward_function_linear(False, t), reward_function_linear(True, t),) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa49868b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 -200\n",
      "10 10 -190\n",
      "20 20 -180\n",
      "30 30 -170\n",
      "40 40 -160\n",
      "50 50 -150\n",
      "60 60 -140\n",
      "70 70 -130\n",
      "80 80 -120\n",
      "90 90 -110\n",
      "100 100 100\n",
      "110 110 110\n",
      "120 120 120\n",
      "130 130 130\n",
      "140 140 140\n",
      "150 150 150\n",
      "160 160 160\n",
      "170 170 170\n",
      "180 180 180\n",
      "190 190 190\n",
      "200 200 200\n",
      "210 210 210\n"
     ]
    }
   ],
   "source": [
    "def reward_function_linear2(done, t, max_reward=200):\n",
    "    reward = 0\n",
    "\n",
    "    if done:\n",
    "        if t >= (max_reward /2) :\n",
    "            reward = t  # discounted steps\n",
    "        else:             \n",
    "            reward = -(max_reward - t)\n",
    "    else:\n",
    "        reward = t\n",
    "\n",
    "    return reward \n",
    "for t in range(0, 220, 10):\n",
    "    print (t, reward_function_linear2(False, t), reward_function_linear2(True, t),) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed5615a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f19308a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_964998/1971341430.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_states_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m durations = fit_networks(policy_net, target_net, env, get_screen, \n\u001b[0m\u001b[1;32m      7\u001b[0m                              \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                              episode_durations=episode_durations)\n",
      "\u001b[0;32m/mnt/data/projects/pytorch-tutorials/notebooks/../wompth/models/dqn.py\u001b[0m in \u001b[0;36mfit_networks\u001b[0;34m(policy_net, target_net, env, get_screen, num_episodes, episode_durations, reward_function)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;31m# Observe new state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mlast_screen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_screen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mcurrent_screen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_screen\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_screen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_964998/2052342902.py\u001b[0m in \u001b[0;36mget_screen\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Resize, and add a batch dimension (BCHW)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input type {} is not supported'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2847\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2849\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2787\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2789\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombytes\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2734\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2735\u001b[0;31m     \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2736\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombytes\u001b[0;34m(self, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_getdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = conf.MAX_EPISODES\n",
    "# restart policy net\n",
    "policy_net = DQN_BackBone(conf=conf)\n",
    "policy_net.load_states_from(target_net)\n",
    "\n",
    "durations = fit_networks(policy_net, target_net, env, get_screen, \n",
    "                             num_episodes=num_episodes, \n",
    "                             episode_durations=episode_durations)\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9238b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average_pth(episode_durations[:1000], 2).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e84c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa63a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f8008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "last_screen = get_screen()\n",
    "current_screen = get_screen()\n",
    "state = current_screen - last_screen\n",
    "for t in count():\n",
    "    # Select and perform an action\n",
    "    action = policy_net.select_action(state)\n",
    "    _, reward, done, _ = env.step(action.item())\n",
    "\n",
    "    # Observe new state\n",
    "    last_screen = current_screen\n",
    "    current_screen = get_screen()\n",
    "    if not done:\n",
    "        next_state = current_screen - last_screen\n",
    "    else:\n",
    "        next_state = None\n",
    "    \n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "        episode_durations.append(t + 1)\n",
    "        print(t)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
