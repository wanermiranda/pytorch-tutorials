{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56739ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import sys \n",
    "sys.path.append('../')\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "275b8e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wompth.models.dqn import Transition, ReplayMemory, DQN, ScreenDims,LayerConf, DQNConf, fit_networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a5ec6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gorigan/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width)\n",
    "    slice_range = slice(view_width)\n",
    "#     cart_location = get_cart_location(screen_width)\n",
    "#     if cart_location < view_width // 2:\n",
    "#         slice_range = slice(view_width)\n",
    "#     elif cart_location > (screen_width - view_width // 2):\n",
    "#         slice_range = slice(-view_width, None)\n",
    "#     else:\n",
    "#         slice_range = slice(cart_location - view_width // 2,\n",
    "#                             cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3577f3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 40, 150])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_screen().cpu().squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d1b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f36bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "screen_dims = ScreenDims(screen_height, screen_width)\n",
    "network_layout = [\n",
    "    LayerConf(input=3, kernel_size=5, stride=2, batch_norm=32), # 3 channels\n",
    "    LayerConf(input=32, kernel_size=5, stride=2, batch_norm=64),\n",
    "    LayerConf(input=64, kernel_size=5, stride=2, batch_norm=32),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e3d1489",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = DQNConf(\n",
    "    BATCH_SIZE = 128,\n",
    "    GAMMA = 0.99,\n",
    "    EPS_START = 1.0,\n",
    "    EPS_MIN = 0.0001,\n",
    "    EPS_DECAY = 0.999,\n",
    "    TARGET_UPDATE = 10, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bacb63a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "target_net = DQN(conf=conf, layout=network_layout, screen_dims=screen_dims, outputs=n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71d88ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_net._linear_input_size, target_net._epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cf7bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# target_net.load_state_dict(policy_net.state_dict())\n",
    "# target_net.eval()\n",
    "\n",
    "\n",
    "def moving_average_pth(x, w=10):\n",
    "    kernel = [1/w] * w\n",
    "    ts_tensor = torch.Tensor(x).reshape(1, 1, -1)\n",
    "    kernel_tensor = torch.Tensor(kernel).reshape(1, 1, -1)\n",
    "    return F.conv1d(ts_tensor, kernel_tensor).reshape(-1)\n",
    "\n",
    "def plot_durations(i_episode, episode_durations):\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel(f'Episode {i_episode}')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    means = moving_average_pth(durations_t, conf.TARGET_UPDATE)\n",
    "    plt.plot(means.numpy())\n",
    "    display.display(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58720399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 -200\n",
      "10 30 -190\n",
      "20 50 -180\n",
      "30 70 -170\n",
      "40 90 -160\n",
      "50 110 -150\n",
      "60 130 -140\n",
      "70 150 -130\n",
      "80 170 -120\n",
      "90 190 -110\n",
      "100 210 -100\n",
      "110 230 -90\n",
      "120 250 -80\n",
      "130 270 -70\n",
      "140 290 -60\n",
      "150 310 -50\n",
      "160 330 -40\n",
      "170 350 -30\n",
      "180 370 -20\n",
      "190 390 -10\n",
      "200 410 200\n",
      "210 430 210\n"
     ]
    }
   ],
   "source": [
    "def reward_function_step(done, t, step_reward=10, max_reward=200):\n",
    "    reward = 0\n",
    "\n",
    "    if done:\n",
    "        if t >= max_reward:\n",
    "            reward = t  # discounted steps\n",
    "        else: \n",
    "            reward = -(max_reward - t)\n",
    "    elif t >= step_reward: \n",
    "        step_bonus = ((t // step_reward)+1)*10\n",
    "        reward = step_bonus + t # promote the reward in steps \n",
    "    else: \n",
    "        reward = t\n",
    "\n",
    "    return reward \n",
    "for t in range(0, 220, 10):\n",
    "    print (t, reward_function_step(False, t), reward_function_step(True, t),) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92d19449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 -200\n",
      "10 10 -190\n",
      "20 20 -180\n",
      "30 30 -170\n",
      "40 40 -160\n",
      "50 50 -150\n",
      "60 60 -140\n",
      "70 70 -130\n",
      "80 80 -120\n",
      "90 90 -110\n",
      "100 100 -100\n",
      "110 110 -90\n",
      "120 120 -80\n",
      "130 130 -70\n",
      "140 140 -60\n",
      "150 150 -50\n",
      "160 160 -40\n",
      "170 170 -30\n",
      "180 180 -20\n",
      "190 190 -10\n",
      "200 200 200\n",
      "210 210 210\n"
     ]
    }
   ],
   "source": [
    "def reward_function_linear(done, t, max_reward=200):\n",
    "    reward = 0\n",
    "\n",
    "    if done:\n",
    "        if t >= max_reward:\n",
    "            reward = t  # discounted steps\n",
    "        else: \n",
    "            reward = -(max_reward - t)\n",
    "    else:\n",
    "        reward = t\n",
    "\n",
    "    return reward \n",
    "for t in range(0, 220, 10):\n",
    "    print (t, reward_function_linear(False, t), reward_function_linear(True, t),) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a665913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 -200\n",
      "10 10 -190\n",
      "20 20 -180\n",
      "30 30 -170\n",
      "40 40 -160\n",
      "50 50 -150\n",
      "60 60 -140\n",
      "70 70 -130\n",
      "80 80 -120\n",
      "90 90 -110\n",
      "100 100 -100\n",
      "110 110 -90\n",
      "120 120 -80\n",
      "130 130 -70\n",
      "140 140 -60\n",
      "150 150 -50\n",
      "160 160 -40\n",
      "170 170 -30\n",
      "180 180 -20\n",
      "190 190 -10\n",
      "200 200 200\n",
      "210 210 210\n"
     ]
    }
   ],
   "source": [
    "def reward_function_linear2(done, t, max_reward=200):\n",
    "    reward = 0\n",
    "\n",
    "    if done:\n",
    "        if t >= (max_reward /2) :\n",
    "            reward = t  # discounted steps\n",
    "        else:             \n",
    "            reward = -(max_reward - t)\n",
    "    else:\n",
    "        reward = t\n",
    "\n",
    "    return reward \n",
    "for t in range(0, 220, 10):\n",
    "    print (t, reward_function_linear(False, t), reward_function_linear(True, t),) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed5615a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f19308a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 15:27:26.803870: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-03 15:27:26.803894: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_841466/828365029.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPS_START\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m durations = DQN.fit_networks(policy_net, target_net, env, get_screen, \n\u001b[0m\u001b[1;32m      8\u001b[0m                              \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                              \u001b[0mepisode_durations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisode_durations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/projects/pytorch-tutorials/notebooks/../wompth/models/dqn.py\u001b[0m in \u001b[0;36mfit_networks\u001b[0;34m(policy_net, target_net, env, get_screen, num_episodes, episode_durations, reward_function)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0;31m# Select and perform an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/projects/pytorch-tutorials/notebooks/../wompth/models/dqn.py\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0;31m# pick action with the larger expected reward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             return torch.tensor(\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/projects/pytorch-tutorials/notebooks/../wompth/models/dqn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mbatch_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_stack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "# restart policy net\n",
    "policy_net = DQN(conf=conf, layout=network_layout, screen_dims=screen_dims, outputs=n_actions)\n",
    "policy_net.load_states_from(target_net)\n",
    "policy_net._conf.EPS_START = 0.01\n",
    "\n",
    "durations = fit_networks(policy_net, target_net, env, get_screen, \n",
    "                             num_episodes=num_episodes, \n",
    "                             episode_durations=episode_durations, \n",
    "                            reward_function=reward_function_linear2)\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9238b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average_pth(episode_durations[:1000], 2).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e84c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa63a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9a2e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "last_screen = get_screen()\n",
    "current_screen = get_screen()\n",
    "state = current_screen - last_screen\n",
    "for t in count():\n",
    "    # Select and perform an action\n",
    "    action = policy_net.select_action(state)\n",
    "    _, reward, done, _ = env.step(action.item())\n",
    "\n",
    "    # Observe new state\n",
    "    last_screen = current_screen\n",
    "    current_screen = get_screen()\n",
    "    if not done:\n",
    "        next_state = current_screen - last_screen\n",
    "    else:\n",
    "        next_state = None\n",
    "    \n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "        episode_durations.append(t + 1)\n",
    "        print(t)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3857fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
