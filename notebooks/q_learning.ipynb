{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56739ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import sys \n",
    "sys.path.append('../')\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "275b8e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wompth.models.dqn import Transition, ReplayMemory, DQN, ScreenDims, DQNConf, fit_networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a5ec6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gorigan/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 80, 150])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(80, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.5)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "\n",
    "#     screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()\n",
    "get_screen().cpu().squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3577f3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d1b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8f36bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "screen_dims = ScreenDims(screen_height, screen_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ea22d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential(\n",
    "#     nn.Conv2d(3,32,kernel_size=3,padding=1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Conv2d(32,64,kernel_size=3,stride=1,padding=1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(2,2),\n",
    "\n",
    "#     nn.Conv2d(64,128,kernel_size=3,stride=1,padding=1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Conv2d(128,128,kernel_size=3,stride=1,padding=1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(2,2),\n",
    "\n",
    "#     nn.Conv2d(128,256,kernel_size=3,stride=1,padding=1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Conv2d(256,256,kernel_size=3,stride=1,padding=1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(2,2),\n",
    "\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(256*4*4,1024),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(1024,512),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(512,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "847e7d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential = nn.Sequential(\n",
    "#       nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5),\n",
    "#       nn.ReLU(),\n",
    "#       nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#       nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n",
    "#       nn.ReLU(),\n",
    "#       nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#       nn.Flatten(start_dim=1)  ,\n",
    "#       nn.Linear(in_features=15232, out_features=120),\n",
    "#       nn.ReLU(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2970f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential(get_screen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b80141db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 80, 150])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_screen().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8ebb6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "class DQN_BackBone(DQN): \n",
    "    def __init__(\n",
    "        self,\n",
    "        device=\"cuda\",\n",
    "        conf: DQNConf = DQNConf(),\n",
    "        optimizer_partial=partial(optim.RMSprop),\n",
    "        memory=ReplayMemory(10000),\n",
    "        screen_dims=ScreenDims(height=40, width=150),\n",
    "        outputs=2\n",
    "    ):\n",
    "        super().__init__(device, conf, optimizer_partial, memory, outputs)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 32, kernel_size=5, stride=2)        \n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(screen_dims.width)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(screen_dims.height)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=linear_input_size, out_features=self._outputs)\n",
    "#         self.out = nn.Linear(in_features=128, out_features=self._outputs)\n",
    "\n",
    "        self._send_to_device(device=device)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(self._device)\n",
    "       \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        return self.fc1(x.view(x.size(0), -1))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e3d1489",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = DQNConf(\n",
    "    BATCH_SIZE = 128,\n",
    "    GAMMA = 0.999,\n",
    "    EPS_START = 0.3,\n",
    "    TARGET_UPDATE = 10, \n",
    "    MAX_EPISODES = 3000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe9ec64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bacb63a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "w, h= get_screen().shape[2:]\n",
    "target_net = DQN_BackBone(conf=conf, screen_dims=ScreenDims(height=h, width=w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df537280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0489, -0.1771]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_net(get_screen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71d88ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_net._linear_input_size, target_net._epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cf7bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# target_net.load_state_dict(policy_net.state_dict())\n",
    "# target_net.eval()\n",
    "\n",
    "\n",
    "def moving_average_pth(x, w=10):\n",
    "    kernel = [1/w] * w\n",
    "    ts_tensor = torch.Tensor(x).reshape(1, 1, -1)\n",
    "    kernel_tensor = torch.Tensor(kernel).reshape(1, 1, -1)\n",
    "    return F.conv1d(ts_tensor, kernel_tensor).reshape(-1)\n",
    "\n",
    "def plot_durations(i_episode, episode_durations):\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel(f'Episode {i_episode}')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    means = moving_average_pth(durations_t, conf.TARGET_UPDATE)\n",
    "    plt.plot(means.numpy())\n",
    "    display.display(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58720399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 -200\n",
      "10 30 -190\n",
      "20 50 -180\n",
      "30 70 -170\n",
      "40 90 -160\n",
      "50 110 -150\n",
      "60 130 -140\n",
      "70 150 -130\n",
      "80 170 -120\n",
      "90 190 -110\n",
      "100 210 -100\n",
      "110 230 -90\n",
      "120 250 -80\n",
      "130 270 -70\n",
      "140 290 -60\n",
      "150 310 -50\n",
      "160 330 -40\n",
      "170 350 -30\n",
      "180 370 -20\n",
      "190 390 -10\n",
      "200 410 200\n",
      "210 430 210\n"
     ]
    }
   ],
   "source": [
    "def reward_function_step(done, t, step_reward=10, max_reward=200):\n",
    "    reward = 0\n",
    "\n",
    "    if done:\n",
    "        if t >= max_reward:\n",
    "            reward = t  # discounted steps\n",
    "        else: \n",
    "            reward = -(max_reward - t)\n",
    "    elif t >= step_reward: \n",
    "        step_bonus = ((t // step_reward)+1)*10\n",
    "        reward = step_bonus + t # promote the reward in steps \n",
    "    else: \n",
    "        reward = t\n",
    "\n",
    "    return reward \n",
    "for t in range(0, 220, 10):\n",
    "    print (t, reward_function_step(False, t), reward_function_step(True, t),) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9bd0b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 -200\n",
      "10 10 -190\n",
      "20 20 -180\n",
      "30 30 -170\n",
      "40 40 -160\n",
      "50 50 -150\n",
      "60 60 -140\n",
      "70 70 -130\n",
      "80 80 -120\n",
      "90 90 -110\n",
      "100 100 100\n",
      "110 110 110\n",
      "120 120 120\n",
      "130 130 130\n",
      "140 140 140\n",
      "150 150 150\n",
      "160 160 160\n",
      "170 170 170\n",
      "180 180 180\n",
      "190 190 190\n",
      "200 200 200\n",
      "210 210 210\n"
     ]
    }
   ],
   "source": [
    "def reward_function_linear2(done, t, max_reward=200):\n",
    "    reward = 0\n",
    "\n",
    "    if done:\n",
    "        if t >= (max_reward /2) :\n",
    "            reward = t  # discounted steps\n",
    "        else:             \n",
    "            reward = -(max_reward - t)\n",
    "    else:\n",
    "        reward = t\n",
    "\n",
    "    return reward \n",
    "for t in range(0, 220, 10):\n",
    "    print (t, reward_function_linear2(False, t), reward_function_linear2(True, t),) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed5615a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19308a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-04 08:12:15.600452: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-04 08:12:15.600476: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "num_episodes = conf.MAX_EPISODES\n",
    "# restart policy net\n",
    "policy_net = DQN_BackBone(conf=conf, screen_dims=ScreenDims(height=h, width=w))\n",
    "policy_net.load_states_from(target_net)\n",
    "\n",
    "durations = fit_networks(policy_net, target_net, env, get_screen, \n",
    "                             num_episodes=num_episodes, \n",
    "                             episode_durations=episode_durations)\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9238b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average_pth(episode_durations[:1000], 2).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e84c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa63a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6805c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "last_screen = get_screen()\n",
    "current_screen = get_screen()\n",
    "state = current_screen - last_screen\n",
    "for t in count():\n",
    "    # Select and perform an action\n",
    "    action = policy_net.select_action(state)\n",
    "    _, reward, done, _ = env.step(action.item())\n",
    "\n",
    "    # Observe new state\n",
    "    last_screen = current_screen\n",
    "    current_screen = get_screen()\n",
    "    if not done:\n",
    "        next_state = current_screen - last_screen\n",
    "    else:\n",
    "        next_state = None\n",
    "    \n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "        episode_durations.append(t + 1)\n",
    "        print(t)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
